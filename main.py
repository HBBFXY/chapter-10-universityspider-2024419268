# å®éªŒä¹ ç½‘ç»œçˆ¬è™«ä¸è‡ªåŠ¨åŒ–ï¼šè½¯ç§‘ä¸­å›½å¤§å­¦æ’å2024å‰30åçˆ¬å–ï¼ˆæœ€ç»ˆå®Œæ•´ç‰ˆï¼‰
import requests
from bs4 import BeautifulSoup
import csv
import time
def get_html_content():
    """è·å–æ’åé¡µé¢HTMLå†…å®¹ï¼ˆç¨³å®šè®¿é—®ï¼‰"""
    url = "https://www.shanghairanking.cn/rankings/bcur/2024"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    try:
        time.sleep(1)
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        response.encoding = "utf-8"
        print("âœ… é¡µé¢è®¿é—®æˆåŠŸï¼")
        return response.text
    except Exception as e:
        print(f"âŒ é¡µé¢è®¿é—®å¤±è´¥ï¼š{str(e)}")
        return None
def parse_rank_data(html):
    """è§£ææ•°æ®ï¼ˆä¿®å¤åç§°æå–é€»è¾‘ï¼‰"""
    if not html:
        return []
    soup = BeautifulSoup(html, "html.parser")
    rank_data = []
    # å®šä½æ’åè¡¨æ ¼
    table = soup.find("table", class_="rk-table")
    if not table:
        print("âŒ æœªæ‰¾åˆ°æ’åè¡¨æ ¼")
        return []
    # æå–å‰30è¡Œæ•°æ®ï¼ˆè·³è¿‡è¡¨å¤´ï¼‰
    rows = table.find_all("tr")[1:31]
    for idx, row in enumerate(rows, 1):
        try:
            cells = row.find_all("td")
            # æ ¸å¿ƒä¿®å¤ï¼šå®šä½class="univ-name"çš„divæå–å¤§å­¦åç§°
            name_elem = cells[1].find("div", class_="univ-name")
            name = name_elem.text.strip() if name_elem else cells[1].text.strip()
            # å…¶ä»–å­—æ®µæå–
            province = cells[2].text.strip() if len(cells)>=3 else "æœªçŸ¥çœå¸‚"
            type_ = cells[3].text.strip() if len(cells)>=4 else "æœªçŸ¥ç±»å‹"
            score = cells[4].text.strip().replace(",", "") if len(cells)>=5 else "0.0"
            
            rank_data.append({
                "æ’å": idx,
                "å¤§å­¦åç§°": name,
                "çœå¸‚": province,
                "ç±»å‹": type_,
                "æ€»åˆ†": score
            })
            print(f"âœ… è§£æç¬¬{idx}åï¼š{name} | {province} | {type_} | æ€»åˆ†ï¼š{score}")
        except Exception as e:
            print(f"âŒ è§£æç¬¬{idx}åå¤±è´¥ï¼š{str(e)}")
            continue
    return rank_data

def save_to_csv(data):
    """ä¿å­˜ä¸ºCSVæ–‡ä»¶"""
    if not data:
        print("âŒ æ— æ•°æ®å¯ä¿å­˜ï¼")
        return
    save_path = "è½¯ç§‘ä¸­å›½å¤§å­¦æ’å2024_å‰30å.csv"
    headers = ["æ’å", "å¤§å­¦åç§°", "çœå¸‚", "ç±»å‹", "æ€»åˆ†"]
    try:
        with open(save_path, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            writer.writeheader()
            writer.writerows(data)
        print(f"\nğŸ‰ æ•°æ®ä¿å­˜æˆåŠŸï¼æ–‡ä»¶è·¯å¾„ï¼š{save_path}")
        print(f"ğŸ“Š å…±ä¿å­˜{len(data)}æ¡æ•°æ®")
    except PermissionError:
        print("âŒ ä¿å­˜å¤±è´¥ï¼šè¯·å…³é—­å·²æ‰“å¼€çš„CSVæ–‡ä»¶ï¼")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥ï¼š{str(e)}")

# ä¸»ç¨‹åºæ‰§è¡Œ
if __name__ == "__main__":
    print("===== å®éªŒä¹ ç½‘ç»œçˆ¬è™«ä¸è‡ªåŠ¨åŒ– å¼€å§‹æ‰§è¡Œ =====")
    html_content = get_html_content()
    rank_result = parse_rank_data(html_content)
    save_to_csv(rank_result)
    print("===== å®éªŒä¹ ç½‘ç»œçˆ¬è™«ä¸è‡ªåŠ¨åŒ– æ‰§è¡Œç»“æŸ =====")
# åœ¨è¿™é‡Œç¼–å†™ä»£ç 
